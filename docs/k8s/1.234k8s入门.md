# k8s

## k8s运行镜像
### 创建docker镜像

使用docker run ，会先检查本地有没有镜像文件，没有镜像文件，会去hub.docker.com拉取镜像，镜像安装，默认拉取的镜像tag为latest版本

```bash
docker run busybox echo ”Hello world”
```

### 创建一个简单的node.js应用

新建kubia目录，新建app.js/Dockerfile 文件
```js
const http = require('http');
const os = require('os');
console.log('kubia server is starting ....');

var handler = function(req,rep){
    console.log("Recei ved request from "+ req.connection. remoteAddress);
    rep.writeHead(200);
    rep.end("youne hit "+os.hostname());
};

var www=http.createServer(handler);
www.listen(8080);
```
在docker中，是分层的，每一条命令都会被docker分为一层，相同的层，docker会互用，提升性能

```c
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node","app.js"]
```

使用命令 `docker build -t kubia .`构建docker镜像

使用命令 `docker images`查看本机所有的镜像，使用`docker ps /docker inspect [name]`查看运行的docker镜像

运行我们打包的kubia镜像`docker run --name kubia-container -p 8080:8080 -d kubia`

容器运行后，浏览器访问`localhost:8080`，就可以看到我们已经启动了应用

使用`docker exec -it kubia-container bash`进入容器内部

`kubectl -n huanghuanghui exec -it pod-demo  --container myapp -- sh` 进入k8s容器内部（多容器）

`docker stop kubia-container /docker rm kubia-container`移除镜像

使用docker ID 登陆后，`docker tag kubia huanghuanghui/kubia`给镜像打tag后`docker push huanghuanghui/kubia`推到远程仓库

### k8s允许镜像
我们上面推送了docker镜像到docker hub中，可以直接使用huanghuanghui/kubia在k8s中启动当前应用


`kubectl run kubia --image=huanghuanghui/kubia --port=8080 --generator=run/vl`

- --image=huanghuanghui/kubia指需要运行的镜像
- --port=8080 告诉k8s监听8080端口
- --generator告诉k8s创建一个ReplicationController（扩缩容管理器）

## k8s pod
中文释义为豆荚，豆荚中包的豆子就是我们的容器，一个pod中可以有多个容器，但是一般一个pod中就只部署一个容器，为了扩缩容考虑，只有多个容器关联非常紧密，才会在一个pod创建多个容器，例如redis的哨兵与它的主节点。


## k8s常用命令

### 本地连接tce

> kubectl get pods 获取所有pod
> kubectl get no 获取k8s节点信息
> kubectl expose rc kubia --type=LoadBalancer --name kubia-http 暴露LoadBlanence到公网

将tce k8s集群通过本地kubelet 连上（复制Kubeconfig文件到本地）
>cd .kube/
>ls
>rm -rf cache/
>vim config 

### 标签
通过配置文件创建pod
>kubectl create -f kubia-manual.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  labels:
    creation_method: manual
    env: pord
spec:
  containers:
  - name: kubia
    image: huanghuanghui/kubia
    ports:
      - containerPort: 8080
        protocol: TCP

```

修改现有服务标签
>kubectl label po kubia-manual.yaml creation_method=manual

改动现有标签
>kubectl label po kubia-manual-v2 env=debug --overwrite

查看k8s容器日志
>kubectl logs [podname]

查看pod标签
>kubectl get pod --show-labels

获取感兴趣的labels，-L大写，代表获取查看所有pod下，将creation_method,env label展示出来
>kubectl get pod -L creation_method,env

通过标签选择器列出pod子集
标签选择器根据资源的以下条件来选择资源:
• 包含(或不包含)使用特定键的标签
• 包含具有特定键和值的标签
• 包含具有特定键的标签， 但其值与我们指定的不同

使用标签分类工作节点
>kubectl label node 10.0.76.8 gpu=true
>kubectl get nodes -l gpu=true
>kubectl get nodes -L gpu

将pod调度到特定节点
>spec:
>  nodeSelector:
>    gpu: "true"

查看pod所有label
>kubectl get pods --show-labels
### namespace 
在使用多个 namespace 的前提下，我们可以将包含大量组件的复杂系统拆分为 更小的不同组，dev/sit/pord

获取所有ns
>kubectl get ns

找到指定ns的资源
>kubectl get po --namespace kube-system
>kubectl get po --n kube-system

通过配置文件创建ns

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespacea
```
命令直接创建：
>create ns custom-ns2

在ns下创建pod
>kubectl create -f kubia-manal-with-gpu.yaml -n sit

### 停止与移除pod
按名称删除
>kubectl delete po kubia-s4xcs

根据标签删除
>kubectl delete po -l creation_method=manual

通过删除整个命名空间来删除 pod
>kubectl delete ns custom-namespace

删除所有pod
>kubectl delete --all pod

如果使用replication Controller，副本控制器，设置了--replicas=3，副本数量，在pod被删除后，还会重新被拉起，需要删除replication Controller

删除所有资源
>kubectl delete all --all

## Probe
### livenessProbe
存活探针，k8s判断容器是否存活，http/tcp/exec command 三种方式。需要配置initialDelaySeconds，容器启动后再检查，不然后被k8s认为容器挂了，会无限重启
```yaml
      livenessProbe:
        httpGet:
          path: /
          port: 8080
        initialDelaySeconds: 15
```

## ReplicationController

创建rc
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-rc
spec:
  replicas: 3
  selector:
    app: kubia-rc
  template:
    metadata:
      name: kubia-rc
      labels:
        app: kubia-rc
    spec:
      containers:
        - name: kubia-rc
          image: huanghuanghui/kubia
          ports:
            - containerPort: 8080

```
### 将pod移入移出rc控制
>
>kubectl label pod kubia-rc-92m7d app=foo --overwrite 重写标签

修改rc配置文件，会将当前rc管理下的pod移出rc管理，然后重新拉起对应label服务
>kubectl edit rc kubia-rc

### 水平伸缩pod
>kubectl scale rc kubia-rc --replicas=10
或者
>kubectl edit re kubia 
修改replicas数量

### 删除rc
删除rc会导致pod也被删除，如果想rc被删除，pod不被删除，可以添加命令
>kubectl delete rc kubia-rc --cascade=false

## ReplicaSet
rs是增强了pod选择器的rc，rs比rc最重要的点是，在pod部署过程中，rc如果出现网络抖动，会出现游离的pod，rs不会

新增了matchLabels标签
```yaml
apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia-rs
  template:
    metadata:
      name: kubia-rs
      labels:
        app: kubia-rs
    spec:
      containers:
        - name: kubia-rs
          image: huanghuanghui/kubia
          ports:
            - containerPort: 8080

```

将rc改为rs可以先删除rc，设置--cascade=false，然后创建label为kubia-rc的rs，rs会自动托管游离的pod


### rs matchExpressions

```yaml
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - "kubia-rs"
          - "kubia-rc"
```
operator
- In : Label的值 必须与其中 一个指定的values 匹配。
- Notln : Label的值与任何指定的values 不匹配。
- Exists : pod 必须包含一个指定名称的标签(值不重要)。使用此运算符时，不应指定 values字段。
- DoesNotExist : pod不得包含有指定名称的标签。values属性不得指定 。

删除ReplicaSet会删除所有的pod
> kubectl delete rs [rsname]

## DaemonSet
需要镜像在每个节点上运行，且每个节点只有一个pod（日志收集），也可以部署在特定节点上

删除/修改node上的标签，会导致pod下线

>kubectl label node minikube disk=ssd

```yaml
apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
```

## 单任务JOB
>kubectl get jobs
>kubectl logs batch-job-tzkg9
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job

```

### CronJob

添加schedule

```yaml
spec:
    schedule: "0,15,30,45 * * * *"
```

## 将本地网络端口转发到 pod 中的端口
可以将将本地网络端口转发到 pod 中的端口，实现pod中代码问题的debug
> kubectl port-forward [podname] 8080:8080

